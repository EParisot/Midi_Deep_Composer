# Midi_Deep_Composer
Use ML to compose music (polyphonic, multitrack, midi input)

# Generated examples:
https://soundcloud.com/klhnikov/sets/ai-lstm-generated-piano-parts

# Magic Piano : play 32 notes, the piano answers...
'''
python Magic_Piano.py
'''

## Data:
For each track, for each note, chord or rest, we used the pitch, duration, offset and velocity as "one-hot encoded" values.

## Model Architecture:
(For each track)
note(s)	-> LSTM(64, return_sequences=True)-> Bidirectional(LSTM(32))-> Dropout	-> 	 -> Dense(notes_vocab)
duration-> LSTM(64, return_sequences=True)-> Bidirectional(LSTM(32))-> Dropout	-> Shared-> Dense(durations_vocab)
offset	-> LSTM(64, return_sequences=True)-> Bidirectional(LSTM(32))-> Dropout	-> Dense -> Dense(offsets_vocab)
velocity-> LSTM(64, return_sequences=True)-> Bidirectional(LSTM(32))-> Dropout	-> 	 -> Dense(velocities_vocab)

We generate a note/chord or rest based on the seq_len (32) last notes/chords or rests. Long chunks of music are generated by sliding the window over time.

[A3, C5, [C2, E2, A2], ...] (seq_len notes) -> predicted note