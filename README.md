# Midi_Deep_Composer
Use ML to compose music (polyphonic, multitrack, midi input)

# Generated examples:
<iframe width="100%" height="300" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/playlists/840650213&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true&visual=true"></iframe>

## Data:
For each track, for each note, chord or rest, we used the pitch, duration, offset and velocity as "one-hot encoded" values.

## Model Architecture:
(For each track)
note(s)		-> LSTM(64, return_sequences=True)	-> Bidirectional(LSTM(32))	-> Dropout	-> 		-> Dense(notes_vocab)
duration	-> LSTM(64, return_sequences=True)	-> Bidirectional(LSTM(32))	-> Dropout	-> Shared	-> Dense(durations_vocab)
offset		-> LSTM(64, return_sequences=True)	-> Bidirectional(LSTM(32))	-> Dropout	-> Dense	-> Dense(offsets_vocab)
velocity	-> LSTM(64, return_sequences=True)	-> Bidirectional(LSTM(32))	-> Dropout	-> 		-> Dense(velocities_vocab)

We generate a note/chord or rest based on the seq_len (32) last notes/chords or rests. Long chunks of music are generated by sliding the window over time.

[A3, C5, [C2, E2, A2], ...] (seq_len notes) -> predicted note